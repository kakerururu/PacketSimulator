"""Evaluator DEVãƒ¢ãƒ¼ãƒ‰ - ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ç”¨ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

estimator ã® DEVãƒ¢ãƒ¼ãƒ‰ (main_dev.py) ã§å‡ºåŠ›ã•ã‚ŒãŸæ¨å®šãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
- Ground Truth: src2_demo/ground_truth_trajectories.json
- æ¨å®šçµæœ: src2_demo/estimated_trajectories.json
- è©•ä¾¡çµæœ: src2_demo/evaluation/results.json
"""

from .usecase.evaluate_trajectories import evaluate_trajectories, EvaluationConfig
from .infrastructure.demo_json_reader import (
    load_demo_ground_truth_trajectories,
    load_demo_estimated_trajectories
)
from .infrastructure.json_writer import save_evaluation_result
from .infrastructure.logger import save_evaluation_logs


def main_dev():
    """DEVãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œé–¢æ•°ï¼ˆãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰"""
    # ãƒ‘ã‚¹è¨­å®š
    ground_truth_path = "src2_demo/ground_truth_trajectories.json"
    estimated_path = "src2_demo/estimated_trajectories.json"
    output_path = "src2_demo/evaluation/results.json"
    log_dir = "src2_demo/evaluate_log"
    tolerance_seconds = 1200.0  # 20åˆ†

    print("=== è»Œè·¡æ¨å®šã®è©•ä¾¡é–‹å§‹ (DEVãƒ¢ãƒ¼ãƒ‰) ===")
    print("ğŸ“ ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: src2_demo/\n")

    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("[Phase 1] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    print(f"  Ground Truth: {ground_truth_path}")
    try:
        gt_trajectories = load_demo_ground_truth_trajectories(ground_truth_path)
        print(f"  âœ“ {len(gt_trajectories)}å€‹ã®Ground Truthè»Œè·¡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ground_truth_path}")
        return
    except Exception as e:
        print(f"  âœ— Ground Truthèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    print(f"  æ¨å®šçµæœ: {estimated_path}")
    try:
        est_trajectories = load_demo_estimated_trajectories(estimated_path)
        num_est_loaded = len(est_trajectories)
        print(f"  âœ“ {num_est_loaded}å€‹ã®æ¨å®šè»Œè·¡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except FileNotFoundError:
        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {estimated_path}")
        print("  â†’ å…ˆã« estimator ã® DEVãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("    python -m src2.estimator.main_dev")
        return
    except Exception as e:
        print(f"  âœ— æ¨å®šçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 2. è©•ä¾¡å®Ÿè¡Œ
    print(f"\n[Phase 2] è©•ä¾¡å®Ÿè¡Œä¸­...")
    config = EvaluationConfig(tolerance_seconds=tolerance_seconds)
    print(f"  è¨±å®¹èª¤å·®: {config.tolerance_seconds}ç§’ ({config.tolerance_seconds/60:.1f}åˆ†)")

    result = evaluate_trajectories(
        gt_trajectories,
        est_trajectories,
        config,
        ground_truth_file=ground_truth_path,
        estimated_file=estimated_path
    )
    print(f"  âœ“ è©•ä¾¡å®Œäº†")

    # 3. çµæœä¿å­˜
    print(f"\n[Phase 3] çµæœä¿å­˜ä¸­...")
    print(f"  JSONå‡ºåŠ›å…ˆ: {output_path}")
    try:
        save_evaluation_result(result, output_path)
        print(f"  âœ“ JSONä¿å­˜å®Œäº†")
    except Exception as e:
        print(f"  âœ— ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 4. è©•ä¾¡ãƒ­ã‚°å‡ºåŠ›
    print(f"\n[Phase 4] è©•ä¾¡ãƒ­ã‚°ä¿å­˜ä¸­...")
    try:
        log_files = save_evaluation_logs(result, log_dir=log_dir)
        print(f"  âœ“ ãƒ­ã‚°ä¿å­˜å®Œäº†:")
        print(f"    - ã‚µãƒãƒªãƒ¼: {log_files['summary']}")
        print(f"    - ãƒ«ãƒ¼ãƒˆè©•ä¾¡è©³ç´°: {log_files['route_evaluations']}")
    except Exception as e:
        print(f"  âœ— ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # 5. ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n=== è»Œè·¡æ¨å®šã®è©•ä¾¡å®Œäº† (DEVãƒ¢ãƒ¼ãƒ‰) ===")
    print("\nğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼:")
    print(f"   - GTè»Œè·¡æ•°: {result.overall_metrics.total_gt_count}")
    print(f"   - Estè»Œè·¡æ•°: {num_est_loaded} (èª­ã¿è¾¼ã¿)")
    print(f"   - è©•ä¾¡å¯¾è±¡: {result.overall_metrics.total_est_count} (å®Œå…¨ãƒ«ãƒ¼ãƒˆ)")

    print(f"\nğŸ“ˆ ç²¾åº¦æŒ‡æ¨™:")
    print(f"   - MAE: {result.overall_metrics.mae:.3f}")
    print(f"   - RMSE: {result.overall_metrics.rmse:.3f}")
    print(f"   - è¿½è·¡ç‡: {result.overall_metrics.tracking_rate:.1%}")
    print(f"   - ç·çµ¶å¯¾èª¤å·®: {result.overall_metrics.total_absolute_error}äºº")

    # ãƒ«ãƒ¼ãƒˆåˆ¥ã®çµ±è¨ˆ
    print(f"\nğŸ“‹ ãƒ«ãƒ¼ãƒˆåˆ¥çµ±è¨ˆ:")
    print(f"   {'ãƒ«ãƒ¼ãƒˆ':<8} {'GT':>4} {'Est':>4} {'èª¤å·®':>4}")
    print(f"   {'-'*8} {'-'*4} {'-'*4} {'-'*4}")
    sorted_evaluations = sorted(result.stay_evaluations, key=lambda x: x.detector_id)
    for se in sorted_evaluations:
        print(f"   {se.detector_id:<8} {se.gt_count:>4} {se.est_count:>4} {se.error:>4}")


if __name__ == "__main__":
    # python -m src2.evaluator.main_dev
    main_dev()
